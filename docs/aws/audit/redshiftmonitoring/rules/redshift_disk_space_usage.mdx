---
slug: redshift_disk_space_usage
title: Clusters With High Disk Usage Should Be Scaled
sidebar_label: Clusters With High Disk Usage Should Be Scaled
---

### More Info:

AWS Redshift clusters with high disk usage should be scaled to increase their storage capacity.

### Risk Level

Low

### Address

Reliability

### Compliance Standards

CBP



### Triage and Remediation
<Tabs>

<Tab title='Cause'>
### Check Cause
<AccordionGroup>
<Accordion title='Using Console' defaultOpen='true'>
1. Log in to the AWS Management Console and navigate to the Amazon Redshift dashboard. 

2. In the navigation pane, select "Clusters". This will display a list of all your Redshift clusters.

3. Select the cluster you want to check. This will open the cluster details page. 

4. In the "Performance" tab, check the "Disk Space Utilization" graph. If the disk usage is consistently high (above 80-90%), it indicates that the cluster may need to be scaled.
</Accordion>

<Accordion title='Using CLI'>
1. Install and configure AWS CLI: Before you can start, you need to install the AWS CLI on your local machine. You can do this by downloading the appropriate installer from the AWS CLI website. Once installed, you can configure it by running `aws configure` and providing your AWS Access Key ID, Secret Access Key, Default region name, and Default output format.

2. List all Redshift clusters: Use the `describe-clusters` command to list all your Redshift clusters. The command is as follows:

   ```
   aws redshift describe-clusters
   ```
   This command will return a JSON output with details about all your Redshift clusters.

3. Check disk usage: For each cluster, check the `TotalStorageCapacity` and `UsedStorageCapacity` fields in the JSON output. These fields tell you the total disk capacity and the used disk capacity of the cluster respectively. 

   ```
   aws redshift describe-clusters --query 'Clusters[*].[ClusterIdentifier,TotalStorageCapacity,UsedStorageCapacity]'
   ```
   This command will return a list of all clusters along with their total and used storage capacities.

4. Calculate disk usage percentage: You can calculate the disk usage percentage for each cluster by dividing the used storage capacity by the total storage capacity and multiplying the result by 100. If the disk usage percentage is high (for example, above 80%), the cluster may need to be scaled. This can be done using a python script to parse the JSON output and calculate the disk usage percentage.
</Accordion>

<Accordion title='Using Python'>
1. Install the necessary Python libraries: Before you can start writing the script, you need to install the necessary Python libraries. You will need the `boto3` library, which is the Amazon Web Services (AWS) SDK for Python. It allows Python developers to write software that makes use of services like Amazon S3, Amazon EC2, and others. You can install it using pip:

   ```python
   pip install boto3
   ```

2. Set up AWS credentials: You need to configure your AWS credentials. You can do this by creating the credentials file yourself (`~/.aws/credentials`):

   ```python
   [default]
   aws_access_key_id = YOUR_ACCESS_KEY
   aws_secret_access_key = YOUR_SECRET_KEY
   ```

   You may also want to add a default region to the AWS configuration file (`~/.aws/config`):

   ```python
   [default]
   region=us-east-1
   ```

3. Write the Python script: Now you can write a Python script that uses the `boto3` library to interact with Redshift and check for clusters with high disk usage. Here is a basic example:

   ```python
   import boto3

   # Create a Redshift client
   redshift = boto3.client('redshift')

   # Get a list of all clusters
   clusters = redshift.describe_clusters()

   # Iterate over each cluster
   for cluster in clusters['Clusters']:
       # Get the cluster's disk usage
       disk_usage = cluster['StorageUtilized']

       # Check if the disk usage is above a certain threshold
       if disk_usage > THRESHOLD:
           print(f"Cluster {cluster['ClusterIdentifier']} has high disk usage: {disk_usage}")
   ```

   This script gets a list of all Redshift clusters, checks the disk usage of each one, and prints a message if the disk usage is above a certain threshold.

4. Run the script: Finally, you can run the script to check for clusters with high disk usage. If any clusters are found with high disk usage, they will be printed to the console. You can run the script using the Python command:

   ```python
   python check_disk_usage.py
   ```

   Replace `check_disk_usage.py` with the name of your Python script.
</Accordion>

</AccordionGroup>
</Tab>

<Tab title='Remediation'>
### Remediation

<AccordionGroup>
<Accordion title='Using Console' defaultOpen='true'>
To remediate high disk usage in an AWS Redshift cluster using the AWS Management Console, follow these steps:

1. **Sign in to the AWS Management Console**: Go to https://aws.amazon.com/ and sign in to the AWS Management Console using your credentials.

2. **Navigate to Amazon Redshift Console**: In the AWS Management Console, navigate to the Amazon Redshift service by either searching for it in the search bar or locating it under the "Analytics" section.

3. **Select the Redshift Cluster**: From the list of clusters, select the Redshift cluster that is experiencing high disk usage and needs to be scaled.

4. **Modify the Cluster**: Click on the "Cluster" name to access the cluster details. Then, click on the "Modify" button to make changes to the cluster configuration.

5. **Adjust the Node Type or Number of Nodes**: In the Modify Cluster section, you can either increase the node type (for more powerful nodes) or increase the number of nodes in the cluster to address the high disk usage issue. You can do this by selecting a different node type from the dropdown menu or adjusting the number of nodes in the "Cluster Configuration" section.

6. **Apply the Changes**: After making the necessary adjustments to the cluster configuration, scroll down and click on the "Apply Changes" button to apply the changes to the cluster.

7. **Monitor the Cluster**: Once the changes are applied, monitor the cluster to ensure that the disk usage has decreased and the cluster performance has improved.

By following these steps and scaling the AWS Redshift cluster to address the high disk usage, you can effectively remediate the issue and optimize the performance of your Redshift cluster.

#
</Accordion>

<Accordion title='Using CLI'>
To remediate high disk usage in AWS Redshift clusters using AWS CLI, follow these steps:

1. **Check Disk Usage**: First, you need to check the disk usage of your Redshift cluster to identify the extent of the issue. You can use the following AWS CLI command to describe the cluster and get information about its disk usage:
   ```
   aws redshift describe-clusters --cluster-identifier YOUR_CLUSTER_IDENTIFIER
   ```
   Replace `YOUR_CLUSTER_IDENTIFIER` with the actual identifier of your Redshift cluster.

2. **Scale Cluster**: If the disk usage is high, you can scale your Redshift cluster by modifying the cluster to increase the number of nodes or node type. To do this, use the following AWS CLI command:
   ```
   aws redshift modify-cluster --cluster-identifier YOUR_CLUSTER_IDENTIFIER --node-type NEW_NODE_TYPE --number-of-nodes NEW_NUMBER_OF_NODES
   ```
   Replace `YOUR_CLUSTER_IDENTIFIER` with the actual identifier of your Redshift cluster, `NEW_NODE_TYPE` with the new node type you want to use, and `NEW_NUMBER_OF_NODES` with the desired number of nodes.

3. **Monitor Cluster**: After scaling the cluster, monitor the disk usage to ensure that it has decreased to a manageable level. You can use the `describe-clusters` command mentioned in step 1 to periodically check the disk usage.

By following these steps, you can effectively remediate high disk usage in AWS Redshift clusters using AWS CLI.
</Accordion>

<Accordion title='Using Python'>
To remediate high disk usage in AWS Redshift clusters using Python, you can automate the process by periodically checking the disk usage metrics and scaling the cluster up if the disk usage exceeds a certain threshold. Here are the step-by-step instructions to remediate this issue:

1. **Install Boto3**: Boto3 is the AWS SDK for Python, which allows you to interact with AWS services. You can install it using pip:
   ```bash
   pip install boto3
   ```

2. **Create a Python script**: Create a Python script that will check the disk usage of the Redshift cluster and scale it up if the disk usage is above the threshold. Below is a sample script to get you started:

   ```python
   import boto3

   # Initialize the Redshift client
   redshift = boto3.client('redshift')

   # Define the cluster identifier
   cluster_identifier = 'your-redshift-cluster-id'

   # Get the disk usage metrics for the cluster
   response = redshift.describe_cluster_performance(clusterIdentifier=cluster_identifier)

   # Check the disk space usage
   disk_space_usage = response['ClusterPerformance']['DiskSpaceUsage']
   disk_space_threshold = 80  # Define your own threshold here

   if disk_space_usage > disk_space_threshold:
       # Scale up the cluster
       redshift.modify_cluster(ClusterIdentifier=cluster_identifier, NumberOfNodes=2)  # Increase the number of nodes as needed
       print(f"Cluster {cluster_identifier} scaled up due to high disk usage.")
   else:
       print(f"Disk usage for cluster {cluster_identifier} is within the threshold.")

   ```

3. **Set up AWS credentials**: Make sure you have set up your AWS credentials either by exporting them as environment variables or using AWS CLI `aws configure` command.

4. **Schedule the script**: You can schedule the script to run periodically using cron jobs or AWS CloudWatch Events to continuously monitor the disk usage of the Redshift cluster and scale it up when needed.

5. **Monitor the remediation**: After implementing the script, monitor the disk usage of the Redshift cluster regularly to ensure that it stays within the threshold and adjust the threshold or scaling logic if necessary.

By following these steps, you can automate the remediation of high disk usage in AWS Redshift clusters using Python.
</Accordion>

</AccordionGroup>
</Tab>
</Tabs>
### Additional Reading:

- [https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html) 

