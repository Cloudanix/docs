

<Tabs><Tab title='Cause'>
### Check Cause

#### Using Console

1. Log in to the Google Cloud Platform Console: Open your web browser, navigate to the Google Cloud Platform Console (console.cloud.google.com) and sign in with your Google account.

2. Navigate to VPC network: From the main dashboard, click on the navigation menu (three horizontal lines in the upper left-hand corner), scroll down and click on "VPC network" under the "Networking" section.

3. Check Firewall rules: In the VPC network page, click on "Firewall rules" in the left-hand menu. This will display a list of all the firewall rules in your GCP environment.

4. Inspect the rules: Look for any rules that have "0.0.0.0/0" (which means all IP addresses) in the "Source IP ranges" column and "tcp:50070" (which is the default port for Hadoop HDFS) in the "Protocols and ports" column. If such a rule exists, it means that the Hadoop HDFS port is open to the entire internet, which is a misconfiguration.

#### Using CLI

1. Install and authenticate the Google Cloud SDK: The Google Cloud SDK includes the gcloud command-line tool, which is necessary to interact with your GCP resources. You can install it by following the instructions provided by Google Cloud. After installation, authenticate the SDK using the command `gcloud auth login`.

2. List all the firewall rules: Use the following command to list all the firewall rules in your GCP project. This will display a list of all firewall rules, including their associated network, source range, allowed protocols and ports, and other information.

    ```
    gcloud compute firewall-rules list
    ```
   
3. Filter the firewall rules: To check if the Hadoop HDFS port (default is 9000) is open, you need to filter the firewall rules that allow traffic to this port. You can do this by using the `--filter` flag with the `gcloud compute firewall-rules list` command. The filter should specify that the allowed ports include 9000.

    ```
    gcloud compute firewall-rules list --filter="allowed:9000"
    ```

4. Review the output: If the Hadoop HDFS port is open, the command will return the firewall rules that allow traffic to this port. If the command does not return any results, it means that the Hadoop HDFS port is not open.

#### Using Python

1. Import Required Libraries:
   First, you need to import the required libraries in your Python script. You will need the 'socket' library to create a socket and connect to the Hadoop HDFS port.

   ```python
   import socket
   ```

2. Define the Function to Check Port:
   Next, define a function that will check if a port is open. This function will take the IP address and port number as arguments, create a socket, and try to connect to the specified IP and port.

   ```python
   def check_port(ip, port):
       sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
       sock.settimeout(10)
       try:
           sock.connect((ip, port))
           sock.shutdown(socket.SHUT_RDWR)
           return True
       except:
           return False
       finally:
           sock.close()
   ```

3. Call the Function:
   Now, call the function with the IP address of your Hadoop server and the HDFS port (default is 9000). If the function returns True, the port is open; if it returns False, the port is closed.

   ```python
   if check_port('your_hadoop_server_ip', 9000):
       print("Hadoop HDFS port is open.")
   else:
       print("Hadoop HDFS port is closed.")
   ```

4. Run the Script:
   Finally, run the script. If the Hadoop HDFS port is open, it will print "Hadoop HDFS port is open." If the port is closed, it will print "Hadoop HDFS port is closed."

Please note that this script only checks if the port is open or closed. It does not check if the port is being used by Hadoop HDFS or some other service. Also, replace 'your_hadoop_server_ip' with the actual IP address of your Hadoop server.

</Tab>

<Tab title='Remediation'>
### Remediation

#### Using Console

To remediate the misconfiguration "Hadoop HDFS Port Should Not Be Open" for GCP using GCP console, follow the below steps:

1. Log in to your GCP console (https://console.cloud.google.com/).
2. Select the project where the Hadoop HDFS port is open.
3. Click on the "Navigation menu" on the top left corner and select "Compute Engine".
4. In the Compute Engine dashboard, click on "VM instances" to see all the instances.
5. Select the instance where the Hadoop HDFS port is open.
6. Click on the "Edit" button at the top of the page.
7. Scroll down to the "Firewall" section and click on "Management, security, disks, networking, sole tenancy".
8. In the "Firewall" section, click on "Networking".
9. In the "Firewall rules" section, click on the "Edit" button next to the firewall rule that allows the Hadoop HDFS port.
10. In the "Edit firewall rule" dialog box, change the "Action" to "Deny".
11. Click on the "Save" button to save the changes.
12. Verify that the Hadoop HDFS port is no longer open by running a port scan on the instance.

By following these steps, you have successfully remediated the misconfiguration "Hadoop HDFS Port Should Not Be Open" for GCP using GCP console.

#### Using CLI

To remediate the misconfiguration "Hadoop HDFS Port Should Not Be Open" for GCP using GCP CLI, follow these steps:

1. Open the Cloud Shell from the GCP console.

2. Run the following command to list all the instances in your project:

   ```
   gcloud compute instances list
   ```

3. Identify the instance that has the Hadoop HDFS port open.

4. Run the following command to SSH into the instance:

   ```
   gcloud compute ssh [INSTANCE_NAME]
   ```

   Replace [INSTANCE_NAME] with the name of the instance.

5. Once you are logged in to the instance, run the following command to stop the Hadoop HDFS service:

   ```
   sudo systemctl stop hadoop-hdfs-datanode
   ```

6. Run the following command to disable the Hadoop HDFS service:

   ```
   sudo systemctl disable hadoop-hdfs-datanode
   ```

7. Edit the Hadoop HDFS configuration file to remove the port configuration. The configuration file is usually located at `/etc/hadoop/conf/hdfs-site.xml`.

8. Save the changes and exit the editor.

9. Run the following command to start the Hadoop HDFS service:

   ```
   sudo systemctl start hadoop-hdfs-datanode
   ```

10. Finally, run the following command to enable the Hadoop HDFS service to start automatically on boot:

    ```
    sudo systemctl enable hadoop-hdfs-datanode
    ```

11. Exit the SSH session by running the following command:

    ```
    exit
    ```

With these steps, you have successfully remediated the misconfiguration "Hadoop HDFS Port Should Not Be Open" for GCP using GCP CLI.

#### Using Python

To remediate the Hadoop HDFS port being open in GCP using Python, follow these steps:

1. Open the Google Cloud Console and navigate to the project that has the misconfiguration.
2. Select the Compute Engine service from the left-hand menu.
3. Select the VM instance that has the Hadoop HDFS port open.
4. Click on the Edit button at the top of the page.
5. Scroll down to the Firewall section and click on it.
6. Click on the Add Firewall Rule button.
7. In the Name field, enter a name for the firewall rule (e.g., "block-hadoop-hdfs-port").
8. In the Targets field, select the VM instance that has the Hadoop HDFS port open.
9. In the Source IP ranges field, enter the IP addresses or ranges that should be blocked from accessing the Hadoop HDFS port. For example, you can enter "0.0.0.0/0" to block all IP addresses.
10. In the Protocols and ports field, select "Specified protocols and ports" and enter the port number for the Hadoop HDFS port (default is 9000).
11. Click on the Create button to create the firewall rule.

Once the firewall rule is created, it will block all incoming traffic to the Hadoop HDFS port from the specified IP addresses or ranges. This will remediate the misconfiguration and prevent unauthorized access to the Hadoop HDFS port.


</Tab>
</Tabs>