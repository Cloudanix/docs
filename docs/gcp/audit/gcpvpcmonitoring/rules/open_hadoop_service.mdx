---
slug: open_hadoop_service
title: Hadoop HDFS NameNode Metadata Service Port Should Not Be Open
sidebar_label: Hadoop HDFS NameNode Metadata Service Port Should Not Be Open
---

### More Info:

Determines if TCP port 8020 for HDFS NameNode metadata service is open to the public.

### Risk Level

Medium

### Address

Security

### Compliance Standards

CBP



<Tabs><Tab title='Cause'>
### Check Cause

#### Using Console

1. Log in to the Google Cloud Platform Console: Open your web browser, navigate to the Google Cloud Platform Console (console.cloud.google.com), and sign in with your Google account credentials.

2. Navigate to VPC network: From the main dashboard, click on the navigation menu (three horizontal lines in the upper left-hand corner), scroll down and click on "VPC network" under the "Networking" section.

3. Check Firewall rules: In the VPC network page, click on "Firewall rules". This will display a list of all the firewall rules in your network. Look for any rules that allow traffic to the Hadoop HDFS NameNode Metadata Service Port (default port 50070).

4. Inspect the rules: Click on the rule to inspect it. Check the "Targets", "Source filter", "Protocols and ports" sections. If the rule allows traffic from any source (0.0.0.0/0) to the HDFS NameNode Metadata Service Port, then it is a misconfiguration.

#### Using CLI

1. Install and authenticate Google Cloud SDK: Before you can start using the GCP CLI, you need to install the Google Cloud SDK on your local machine and authenticate it. You can download the SDK from the official Google Cloud website and authenticate it using the command `gcloud auth login`.

2. List all the firewall rules: Once you have the Google Cloud SDK set up, you can use the `gcloud` command to list all the firewall rules in your project. The command to do this is `gcloud compute firewall-rules list`. This will give you a list of all the firewall rules along with their details.

3. Filter the firewall rules for HDFS NameNode Metadata Service Port: The default port for HDFS NameNode Metadata Service is 50070. You can filter the firewall rules for this port using the command `gcloud compute firewall-rules list --filter="allowed:tcp:50070"`. This will give you a list of all the firewall rules that allow traffic on port 50070.

4. Check if the firewall rules allow traffic from all IP addresses: If the firewall rules allow traffic from all IP addresses (0.0.0.0/0), then the HDFS NameNode Metadata Service Port is open to the network. You can check this by looking at the source ranges in the output of the previous command. If the source ranges include 0.0.0.0/0, then the port is open to the network.

#### Using Python

Detecting whether the Hadoop HDFS NameNode Metadata Service Port is open in the network involves checking if the port 50070 is open. Here are the steps to do this using Python:

1. Import the necessary Python libraries:
```python
import socket
```

2. Define a function to check if a port is open:
```python
def check_port(ip, port):
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(10)
    try:
        sock.connect((ip, port))
        sock.shutdown(socket.SHUT_RDWR)
        return True
    except:
        return False
    finally:
        sock.close()
```

3. Call the function with the IP address of your Hadoop HDFS NameNode and the port number 50070:
```python
ip = 'your_hadoop_namenode_ip'
port = 50070
if check_port(ip, port):
    print(f'Port {port} is open on {ip}')
else:
    print(f'Port {port} is not open on {ip}')
```

4. Run the script. If the port is open, it will print "Port 50070 is open on your_hadoop_namenode_ip". If it's not open, it will print "Port 50070 is not open on your_hadoop_namenode_ip".

Please replace 'your_hadoop_namenode_ip' with the actual IP address of your Hadoop HDFS NameNode. Also, ensure that the machine where you run this script has network access to the Hadoop HDFS NameNode.

</Tab>


<Tab title='Remediation'>
### Remediation

#### Using Console

To remediate the misconfiguration "Hadoop HDFS NameNode Metadata Service Port Should Not Be Open" for GCP using GCP console, please follow the below steps:

1. Open the GCP console and navigate to the Compute Engine section.

2. Click on the VM instances tab and select the instance where the Hadoop HDFS NameNode Metadata Service Port is open.

3. Click on the Edit button to edit the instance settings.

4. Scroll down to the "Firewall" section and click on the "Network tags" drop-down menu.

5. Add a new network tag and give it a name, for example, "no-namenode-port".

6. Click on the "Save" button to save the changes.

7. Navigate to the "VPC network" section and click on the "Firewall rules" tab.

8. Click on the "Create Firewall Rule" button to create a new firewall rule.

9. Give the firewall rule a name, for example, "no-namenode-port".

10. In the "Targets" section, select "Specified target tags" and enter the tag name "no-namenode-port".

11. In the "Source filter" section, select "IP ranges" and enter the IP address range of the network that should not have access to the Hadoop HDFS NameNode Metadata Service Port.

12. In the "Protocols and ports" section, select "Specified protocols and ports" and enter the protocol and port number of the Hadoop HDFS NameNode Metadata Service Port (default is TCP port 8020).

13. Click on the "Create" button to create the firewall rule.

14. Verify that the firewall rule is applied to the instance by checking the "Firewall rules" section on the instance details page.

15. Test the configuration by attempting to access the Hadoop HDFS NameNode Metadata Service Port from a network that is not allowed. The connection should be refused.

By following these steps, you will remediate the misconfiguration "Hadoop HDFS NameNode Metadata Service Port Should Not Be Open" for GCP using GCP console.

#### Using CLI

To remediate the misconfiguration "Hadoop HDFS NameNode Metadata Service Port Should Not Be Open" for GCP using GCP CLI, please follow the below steps:

1. Open the Cloud Shell from the GCP console.

2. Run the following command to get the list of all the Compute Engine instances in your project:

```
gcloud compute instances list
```

3. Identify the instance where the Hadoop HDFS NameNode Metadata Service Port is open.

4. SSH into the instance using the following command:

```
gcloud compute ssh [INSTANCE_NAME]
```

5. Edit the Hadoop configuration file `hdfs-site.xml` using the following command:

```
sudo nano /etc/hadoop/conf/hdfs-site.xml
```

6. Add the following property to the file:

```
<property>
  <name>dfs.namenode.rpc-bind-host</name>
  <value>127.0.0.1</value>
</property>
```

This will bind the Hadoop HDFS NameNode to the loopback IP address and prevent it from being accessible from the network.

7. Save and exit the file.

8. Restart the Hadoop HDFS NameNode service using the following command:

```
sudo systemctl restart hadoop-hdfs-namenode.service
```

9. Verify that the Hadoop HDFS NameNode Metadata Service Port is no longer open by running the following command:

```
sudo lsof -i :8020
```

This should not return any output.

10. Exit the SSH session using the following command:

```
exit
```

By following these steps, you can remediate the misconfiguration "Hadoop HDFS NameNode Metadata Service Port Should Not Be Open" for GCP using GCP CLI.

#### Using Python

To remediate the misconfiguration "Hadoop HDFS NameNode Metadata Service Port Should Not Be Open" on GCP, you can follow the below steps using Python:

1. First, you need to authenticate with GCP using the Python SDK. You can do this by installing the `google-cloud-sdk` and running the command `gcloud auth application-default login`. This will authenticate you with your GCP account.

2. Next, you need to get a list of all the instances in your GCP project. You can do this by using the `google-cloud-sdk` command `gcloud compute instances list` or by using the Python SDK. Here's the Python code to get a list of all the instances:

```python
from google.cloud import compute_v1

client = compute_v1.InstancesClient()

project = 'your-project-id'

zones = ['us-central1-a', 'us-central1-b', 'us-central1-c']

instances = []

for zone in zones:
    result = client.list(project=project, zone=zone)
    for instance in result:
        instances.append(instance)
```

3. Once you have a list of all the instances, you need to check if the Hadoop HDFS NameNode Metadata Service Port is open on any of them. You can do this by using the `socket` library in Python. Here's the Python code to check if the port is open:

```python
import socket

def is_port_open(ip, port):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect((ip, port))
        s.shutdown(socket.SHUT_RDWR)
        return True
    except:
        return False
    finally:
        s.close()

port = 8020

for instance in instances:
    ip = instance.network_interfaces[0].network_ip
    if is_port_open(ip, port):
        print(f"Port {port} is open on instance {instance.name}.")
```

4. If the Hadoop HDFS NameNode Metadata Service Port is open on any instance, you need to close it. You can do this by creating a firewall rule to block traffic on that port. Here's the Python code to create a firewall rule:

```python
from google.cloud import compute_v1

client = compute_v1.FirewallsClient()

project = 'your-project-id'

firewall_rule_name = 'block-hadoop-port'

network_name = 'default'

direction = 'INGRESS'

priority = 1000

ip_protocol = 'tcp'

port = 8020

target_tags = ['hadoop']

firewall_rule = {
    'name': firewall_rule_name,
    'network': f'projects/{project}/global/networks/{network_name}',
    'direction': direction,
    'priority': priority,
    'sourceRanges': ['0.0.0.0/0'],
    'targetTags': target_tags,
    'allowed': [{
        'IPProtocol': ip_protocol,
        'ports': [str(port)]
    }]
}

operation = client.insert(project=project, firewall_resource=firewall_rule)

print(f"Created firewall rule {firewall_rule_name}.")
```

5. Finally, you need to apply the `hadoop` tag to all the instances running Hadoop. You can do this by using the Python SDK. Here's the Python code to apply the tag:

```python
from google.cloud import compute_v1

client = compute_v1.InstancesClient()

project = 'your-project-id'

zones = ['us-central1-a', 'us-central1-b', 'us-central1-c']

tag = 'hadoop'

for zone in zones:
    result = client.list(project=project, zone=zone)
    for instance in result:
        if 'hadoop' in instance.tags.items:
            continue
        instance.tags.items.append(tag)
        operation = client.update(project=project, zone=zone, instance=instance.name, instance_resource=instance)

print(f"Applied tag {tag} to all Hadoop instances.")
```

By following these steps, you can remediate the misconfiguration "Hadoop HDFS NameNode Metadata Service Port Should Not Be Open" on GCP using Python.


### Additional Reading:

- [https://cloud.google.com/vpc/docs/using-firewalls](https://cloud.google.com/vpc/docs/using-firewalls) 


</Tab>
</Tabs>