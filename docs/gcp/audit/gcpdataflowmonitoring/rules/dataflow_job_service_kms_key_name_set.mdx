---
slug: dataflow_job_service_kms_key_name_set
title: Dataflow Job CMK Keys Should Be Set
sidebar_label: Dataflow Job CMK Keys Should Be Set
---

### More Info:

Ensure Dataflow jobs are encrypted with customer managed kyes

### Risk Level

Low

### Address

Reliability, Security

### Compliance Standards

SOC2, GDPR, ISO27001, HIPAA, HITRUST, NISTCSF, PCIDSS


<Tabs><Tab title='Cause'>
### Check Cause

#### Using Console

1. Log in to the Google Cloud Platform Console: Open your web browser, navigate to the Google Cloud Platform Console (console.cloud.google.com), and sign in with your Google account credentials.

2. Navigate to Dataflow Jobs: From the main dashboard, click on the navigation menu (three horizontal lines) in the upper left-hand corner. Scroll down and click on "Dataflow" under the "BIG DATA" section. This will take you to the Dataflow jobs page.

3. Check Dataflow Job Details: On the Dataflow jobs page, you will see a list of all your Dataflow jobs. Click on the name of the job you want to check. This will take you to the job details page.

4. Check CMK Keys: On the job details page, look for the "Cloud KMS key" field under the "Job configuration" section. If this field is empty or not set, then the Dataflow job does not have a Customer-Managed Key (CMK) set. If the field contains a key, then a CMK is set for the job.

#### Using CLI

1. First, you need to install and initialize the Google Cloud SDK if you haven't done so already. You can download it from the official Google Cloud website and follow the instructions to install it. Once installed, you can initialize it by running the command `gcloud init` and follow the prompts to authorize the SDK and set your default project, compute region, and compute zone.

2. Once the Google Cloud SDK is installed and initialized, you can list all the Dataflow jobs in your project by running the following command:

   ```
   gcloud dataflow jobs list
   ```

   This command will return a list of all Dataflow jobs in your project, along with their job IDs, creation times, job names, types, and statuses.

3. To check the CMK keys for a specific Dataflow job, you need to describe the job using its job ID. You can do this by running the following command:

   ```
   gcloud dataflow jobs describe [JOB_ID]
   ```

   Replace `[JOB_ID]` with the ID of the job you want to check. This command will return detailed information about the job, including its configuration.

4. In the output of the previous command, look for the `kmsKeyName` field under the `environment` section. This field contains the name of the CMK key used by the job. If this field is not present or is empty, it means that the job is not using a CMK key.

#### Using Python

To check if Customer-Managed Encryption Keys (CMEK) are set in Google Cloud Dataflow, you can use the Google Cloud SDK (gcloud) or the Google Cloud Client Libraries for Python. Here are the steps:

1. **Set up Google Cloud SDK and Python Environment:**
   First, you need to install and initialize the Google Cloud SDK. Also, make sure Python and pip (Python package installer) are installed on your system. You can install the Google Cloud Client Libraries for Python using pip:

   ```bash
   pip install --upgrade google-cloud-dataflow
   ```

2. **Authenticate your SDK:**
   Use the following command to authenticate your SDK:

   ```bash
   gcloud auth application-default login
   ```

3. **List Dataflow Jobs and Check CMEK Settings:**
   Use the Dataflow client library to list all Dataflow jobs and check their CMEK settings. Here is a sample Python script:

   ```python
   from google.cloud import dataflow_v1b3 as dataflow

   def check_cmek_settings():
       client = dataflow.DataflowV1b3Client()

       # Replace 'my-project' with your project ID
       project_id = 'my-project'
       location = 'us-central1'  # or any other region

       request = dataflow.ListJobsRequest(project_id=project_id, location=location)
       response = client.list_jobs(request)

       for job in response.jobs:
           if 'kmsKeyName' in job.environment:
               print(f"Job {job.id} has CMEK set to {job.environment['kmsKeyName']}")
           else:
               print(f"Job {job.id} does not have CMEK set")

   if __name__ == "__main__":
       check_cmek_settings()
   ```

4. **Run the Python Script:**
   Save the above script in a file, say `check_cmek.py`, and run it using Python:

   ```bash
   python check_cmek.py
   ```

This script will list all Dataflow jobs in the specified project and location, and print whether each job has CMEK set or not. If a job has CMEK set, it will also print the key name.

</Tab>


<Tab title='Remediation'>
### Remediation

#### Using Console

To remediate the "Dataflow Job CMK Keys Should Be Set" misconfiguration in GCP using the GCP console, please follow these steps:

1. Open the Google Cloud Console and go to the Dataflow page.
2. Select the Dataflow job that is affected by the misconfiguration.
3. Click on the "Edit" button to edit the job configuration.
4. Scroll down to the "Security" section of the configuration page.
5. Under the "Encryption" section, select "Customer-managed key" from the "Key source" dropdown menu.
6. Choose the appropriate Cloud KMS key that you want to use to encrypt your data.
7. Click on the "Save" button to save the changes.

After following these steps, your Dataflow job will be configured to use customer-managed keys for encryption.

#### Using CLI

To remediate the "Dataflow Job CMK Keys Should Be Set" misconfiguration in GCP using GCP CLI, you can follow the below steps:

1. Open the Cloud Shell in your GCP Console.

2. Run the following command to set the Cloud Key Management Service (KMS) key for Dataflow:

   ```
   gcloud dataflow jobs update <JOB_ID> --update-kms-key=<KMS_KEY>
   ```

   Replace `<JOB_ID>` with the ID of the Dataflow job and `<KMS_KEY>` with the ID or fully qualified name of the Cloud KMS key to be used for encrypting the Dataflow job's temporary files.

3. Verify that the KMS key has been set by running the following command:

   ```
   gcloud dataflow jobs describe <JOB_ID> | grep kmsKeyName
   ```

   This command should return the name of the KMS key that was set in step 2.

4. Repeat steps 2 and 3 for all Dataflow jobs that require a KMS key to be set.

By following these steps, you can remediate the "Dataflow Job CMK Keys Should Be Set" misconfiguration in GCP using GCP CLI.

#### Using Python

To remediate the misconfiguration "Dataflow Job CMK Keys Should Be Set" for GCP using python, you can follow the below steps:

1. First, you need to create a Cloud KMS key ring and key in the same region as your Dataflow job. You can use the following code to create a key ring and key:

```python
from google.cloud import kms_v1

# Replace <project-id> with your GCP project ID
# Replace <key-ring-name> with the name of the key ring you want to create
# Replace <key-name> with the name of the key you want to create

project_id = "<project-id>"
key_ring_name = "<key-ring-name>"
key_name = "<key-name>"

# Create the Cloud KMS client
client = kms_v1.KeyManagementServiceClient()

# Create the key ring
parent = f"projects/{project_id}/locations/{location}"
key_ring = client.create_key_ring(request={"parent": parent, "key_ring_id": key_ring_name})

# Create the key
purpose = kms_v1.CryptoKey.CryptoKeyPurpose.ENCRYPT_DECRYPT
crypto_key = {"purpose": purpose}
response = client.create_crypto_key(request={"parent": key_ring.name, "crypto_key_id": key_name, "crypto_key": crypto_key})
```

2. Once you have created the key ring and key, you can update your Dataflow job to use the key. You can use the following code to update your job:

```python
from googleapiclient.discovery import build
from oauth2client.client import GoogleCredentials

# Replace <project-id> with your GCP project ID
# Replace <job-id> with the ID of the Dataflow job you want to update
# Replace <location> with the region where your job is running
# Replace <key-ring-name> with the name of the key ring you created
# Replace <key-name> with the name of the key you created

project_id = "<project-id>"
job_id = "<job-id>"
location = "<location>"
key_ring_name = "<key-ring-name>"
key_name = "<key-name>"

# Authenticate with GCP
credentials = GoogleCredentials.get_application_default()
service = build('dataflow', 'v1b3', credentials=credentials)

# Get the current job configuration
job = service.projects().locations().jobs().get(projectId=project_id, location=location, jobId=job_id).execute()

# Update the job configuration to use the Cloud KMS key
job['environment']['userAgent']['additionalUserAgent'] = 'dataflow-kms-sample'
job['environment']['workerPools'][0]['workerHarnessContainerImage'] = 'gcr.io/dataflow-kms-cloud/dataflow-kms-sample:latest'
job['environment']['workerPools'][0]['environment']['KMS_KEY_NAME'] = f"projects/{project_id}/locations/{location}/keyRings/{key_ring_name}/cryptoKeys/{key_name}"

# Update the job
request = service.projects().locations().jobs().update(projectId=project_id, location=location, jobId=job_id, body=job)
response = request.execute()
```

3. Finally, you can verify that the job is using the Cloud KMS key by checking the logs. You should see a message similar to the following:

```
INFO:root:Using Cloud KMS key projects/<project-id>/locations/<location>/keyRings/<key-ring-name>/cryptoKeys/<key-name> to encrypt data
```

By following these steps, you should be able to remediate the misconfiguration "Dataflow Job CMK Keys Should Be Set" for GCP using python.




</Tab>
</Tabs>