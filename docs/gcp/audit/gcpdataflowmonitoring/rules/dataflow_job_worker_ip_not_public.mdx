---
slug: dataflow_job_worker_ip_not_public
title: Dataflow Worker Ip Should Not Be Public
sidebar_label: Dataflow Worker Ip Should Not Be Public
---

### More Info:

Ensure Dataflow jobs worker ip is not public

### Risk Level

Critical

### Address

Security

### Compliance Standards

CBP, CISGCP, HITRUST, SOC2, GDPR, NISTCSF, PCIDSS


<Tabs><Tab title='Cause'>
### Check Cause

#### Using Console

1. Log in to your Google Cloud Platform (GCP) console.

2. Navigate to the Dataflow section. You can do this by clicking on the navigation menu (three horizontal lines at the top left corner), then scroll down and click on "Dataflow".

3. In the Dataflow section, you will see a list of all your Dataflow jobs. Click on the specific job you want to check.

4. In the job details page, check the "Worker IP Configuration" setting. If it is set to "Public", then the Dataflow worker IP is public. If it is set to "Private", then the Dataflow worker IP is not public.

#### Using CLI

1. First, you need to install and configure the Google Cloud SDK (Software Development Kit) on your local machine. This will allow you to interact with your GCP resources using the command line interface (CLI). You can download the SDK from the official Google Cloud website and follow the instructions to install and initialize it.

2. Once the SDK is installed and configured, you can use the `gcloud` command to interact with your GCP resources. To list all the Dataflow jobs in your project, use the following command:

   ```
   gcloud dataflow jobs list
   ```

   This will return a list of all the Dataflow jobs in your project, along with their IDs, names, types, creation times, statuses, and other information.

3. To get detailed information about a specific Dataflow job, including its worker IP configuration, use the following command:

   ```
   gcloud dataflow jobs describe [JOB_ID]
   ```

   Replace `[JOB_ID]` with the ID of the job you're interested in. This will return a detailed description of the job, including its worker IP configuration.

4. To check if the worker IP is public, look for the `workerPools.ipConfiguration` field in the job description. If this field is set to `WORKER_IP_PUBLIC`, then the worker IP is public. If it's set to `WORKER_IP_PRIVATE`, then the worker IP is private.

#### Using Python

To detect if the Dataflow Worker IP is public in Google Cloud Platform's Data Flow, you can use the Google Cloud SDK (gcloud) or the Google Cloud Client Libraries for Python. Here are the steps:

1. **Install Google Cloud SDK and Python Client Libraries:**
   If not already installed, you need to install the Google Cloud SDK and the Google Cloud Client Libraries for Python. You can do this using pip (Python's package installer). Run the following commands in your terminal:

   ```bash
   pip install --upgrade google-cloud-sdk
   pip install --upgrade google-cloud-dataflow
   ```

2. **Authenticate your SDK:**
   You need to authenticate your SDK to your Google Cloud account. You can do this by running the following command in your terminal and following the prompts:

   ```bash
   gcloud auth login
   ```

3. **List all Dataflow Jobs:**
   Use the Dataflow client library to list all Dataflow jobs and their details. Here is a sample Python script:

   ```python
   from google.cloud import dataflow

   def list_jobs(project_id):
       client = dataflow.DataflowClient()
       jobs = client.list_jobs(project_id=project_id)
       for job in jobs:
           print(job)
   ```

4. **Check if Worker IPs are Public:**
   For each job, check the `workerPools.ipConfiguration` field. If it is set to `WORKER_IP_PUBLIC`, then the worker IP is public. Here is how you can modify the above script to do this:

   ```python
   from google.cloud import dataflow

   def list_jobs(project_id):
       client = dataflow.DataflowClient()
       jobs = client.list_jobs(project_id=project_id)
       for job in jobs:
           for worker_pool in job.workerPools:
               if worker_pool.ipConfiguration == dataflow.WorkerIpConfiguration.WORKER_IP_PUBLIC:
                   print(f"Job {job.id} has public worker IPs")
   ```

This script will print out the IDs of all Dataflow jobs that have public worker IPs. You can modify it to suit your needs, for example by collecting the IDs in a list and returning them, or by raising an exception if any public IPs are found.

</Tab>


<Tab title='Remediation'>
### Remediation

#### Using Console

To remediate the misconfiguration "Dataflow Worker IP should not be public" in GCP, you can follow the below steps using GCP console:

1. Go to the GCP console and navigate to the Dataflow Workers page.
2. Select the worker pool that you want to update.
3. Click on the "Edit" button.
4. In the "Network settings" section, select the "Private" option for the "Worker IP" setting.
5. Click on the "Save" button to save the changes.

By selecting the "Private" option, the Dataflow worker IP will not be exposed to the public internet. This will help to ensure the security of your Dataflow jobs and prevent unauthorized access.

#### Using CLI

To remediate the "Dataflow Worker IP Should Not Be Public" misconfiguration in GCP using GCP CLI, you can follow these step-by-step instructions:

1. Open the Cloud Shell in your GCP console.

2. Run the following command to get the list of all Dataflow jobs:
```
gcloud dataflow jobs list
```

3. Identify the job for which you want to remediate the misconfiguration.

4. Run the following command to update the job with the `--no-use-public-ips` flag:
```
gcloud dataflow jobs update <JOB_ID> --no-use-public-ips
```
Replace `<JOB_ID>` with the actual ID of the job you want to update.

5. Verify that the job has been updated by running the following command:
```
gcloud dataflow jobs describe <JOB_ID> | grep "usePublicIps"
```
This command should return `usePublicIps: false`.

By following these steps, you should be able to remediate the "Dataflow Worker IP Should Not Be Public" misconfiguration in GCP using GCP CLI.

#### Using Python

To remediate the misconfiguration "Dataflow Worker IP Should Not Be Public" in GCP using Python, follow these steps:

1. Open the Google Cloud Console and select the project that you want to work on.
2. Go to the Dataflow section of the console.
3. Select the Dataflow job that you want to remediate.
4. Click on the "Edit" button to edit the job configuration.
5. In the "Networking" section, select "Custom" for the "Worker IP Configuration" option.
6. In the "Custom" section, select "Private" for the "Worker IP" option.
7. Click on the "Save" button to save the changes to the job configuration.

Here is the Python code to remediate the misconfiguration:

```python
from googleapiclient.discovery import build
from google.oauth2 import service_account

# Set the project ID and Dataflow job ID
project_id = 'your-project-id'
job_id = 'your-job-id'

# Set the worker IP configuration to private
body = {
    'environment': {
        'workerIPAddressConfiguration': 'WORKER_IP_PRIVATE'
    }
}

# Authenticate with GCP using a service account key file
credentials = service_account.Credentials.from_service_account_file('path/to/keyfile.json')
dataflow = build('dataflow', 'v1b3', credentials=credentials)

# Update the Dataflow job configuration
request = dataflow.projects().locations().jobs().update(
    projectId=project_id,
    location='us-central1',
    jobId=job_id,
    body=body
)
response = request.execute()

print(response)
```

Note: Replace "your-project-id", "your-job-id", and "path/to/keyfile.json" with your actual project ID, Dataflow job ID, and the path to your service account key file, respectively.




</Tab>
</Tabs>