---
slug: bigquery_user_activity_logging_enabled
title: GCP BigQuery Should Have User Activity Logging Enabled
sidebar_label: GCP BigQuery Should Have User Activity Logging Enabled
---

### More Info:

Ensure that BigQuery User Activity Audit Logging is configured properly across all projects.

### Risk Level

Medium

### Address

Security

### Compliance Standards

CISGCP, CBP, GDPR, HIPAA, ISO27001


### Triage and Remediation
<Tabs>

<Tab title='Cause'>
### Check Cause
<AccordionGroup>
<Accordion title='Using Console' defaultOpen='true'>
1. Log in to the Google Cloud Platform Console: Open your web browser, navigate to the Google Cloud Platform Console (https://console.cloud.google.com/), and sign in with your Google account credentials.

2. Navigate to BigQuery: From the left-hand side menu, select "BigQuery" under the "Big Data" section. This will take you to the BigQuery interface.

3. Check Dataset Settings: In the BigQuery interface, select a dataset from the resources panel on the left. Then, click on the name of the dataset to open its "Dataset details" page. 

4. Verify User Activity Logging: On the "Dataset details" page, click on the "Access" tab. Under this tab, look for the "Cloud Audit Logs" section. If "Data Access" and "Admin Activity" logs are enabled, then user activity logging is properly configured. If not, then there is a misconfiguration.
</Accordion>

<Accordion title='Using CLI'>
1. Install and configure the Google Cloud SDK CLI on your local machine. You can download it from the official Google Cloud website and follow the instructions to set it up.

2. Once the SDK is installed and configured, open your terminal and use the following command to list all the BigQuery datasets in your project:

   ```
   gcloud bigquery datasets list --project=[PROJECT_ID]
   ```
   Replace `[PROJECT_ID]` with your actual project ID.

3. For each dataset listed, check the access control list (ACL) to see if user activity logging is enabled. Use the following command:

   ```
   gcloud bigquery datasets describe [DATASET_ID] --project=[PROJECT_ID]
   ```
   Replace `[DATASET_ID]` and `[PROJECT_ID]` with your actual dataset ID and project ID respectively.

4. In the output of the above command, look for the `access` field. If user activity logging is enabled, you should see an entry like this:

   ```
   access:
   - role: READER
     specialGroup: projectReaders
   - role: WRITER
     specialGroup: projectWriters
   - role: OWNER
     specialGroup: projectOwners
   - role: OWNER
     userByEmail: [USER_EMAIL]
   ```
   If you don't see this entry, it means user activity logging is not enabled for that dataset.
</Accordion>

<Accordion title='Using Python'>
1. Install the necessary Python libraries: Google Cloud libraries are required to interact with GCP services. You can install it using pip:

```python
pip install --upgrade google-cloud-logging
```

2. Import the necessary libraries and initialize the client:

```python
from google.cloud import logging
client = logging.Client()
```

3. Fetch the BigQuery datasets and check for the logging configuration:

```python
def check_bigquery_logging():
    from google.cloud import bigquery
    client = bigquery.Client()

    datasets = list(client.list_datasets())  # Make an API request.
    for dataset in datasets:
        dataset_id = dataset.dataset_id
        dataset = client.get_dataset(dataset_id)  # Make an API request.
        if not dataset.access_entries:
            print(f"Dataset {dataset_id} has no access entries defined.")
        else:
            for entry in dataset.access_entries:
                if entry.role != 'READER' or 'logs-Viewer' not in entry.role:
                    print(f"Dataset {dataset_id} does not have user activity logging enabled.")
```

4. Run the script: Execute the script to check the logging configuration for all BigQuery datasets. If any dataset does not have user activity logging enabled, it will print a message with the dataset ID.

Please note that this script assumes that you have authenticated your GCP account in your environment. If not, you need to authenticate it using `gcloud auth application-default login` before running the script.
</Accordion>

</AccordionGroup>
</Tab>

<Tab title='Remediation'>
### Remediation

<AccordionGroup>
<Accordion title='Using Console' defaultOpen='true'>
To remediate the misconfiguration "GCP BigQuery Should Have User Activity Logging Enabled" for GCP using GCP console, follow the below steps:

1. Open the GCP console and navigate to the BigQuery service.

2. Click on the "Navigation Menu" icon on the top-left corner of the console.

3. From the menu, select "BigQuery".

4. In the BigQuery console, click on the "Settings" icon on the left-hand side panel.

5. Click on the "Audit logs" tab.

6. Under the "Audit logs" tab, click on the "Configure" button.

7. In the "Configure audit logs" window, select the checkbox next to "Data access".

8. Select the checkbox next to "Cloud audit logs".

9. Click on the "Save" button.

10. Once the configuration is saved, the user activity logging for BigQuery is enabled.

Note: It is recommended to create a log sink to export the logs to a centralized logging system for further analysis.

#
</Accordion>

<Accordion title='Using CLI'>
To remediate the misconfiguration "GCP BigQuery Should Have User Activity Logging Enabled" for GCP using GCP CLI, follow the below steps:

Step 1: Open the command prompt or terminal on your local machine.

Step 2: Authenticate to your GCP account using the below command:

```
gcloud auth login
```

Step 3: Set the project to the project for which you want to enable user activity logging using the below command:

```
gcloud config set project [PROJECT_ID]
```

Step 4: Enable the BigQuery API using the below command:

```
gcloud services enable bigquery.googleapis.com
```

Step 5: Enable user activity logging for BigQuery using the below command:

```
gcloud logging sinks create [SINK_NAME] bigquery.googleapis.com/projects/[PROJECT_ID]/datasets/[DATASET_ID] --log-filter='protoPayload.methodName="google.cloud.bigquery.v2.JobService.InsertJob"'
```

Note: Replace [SINK_NAME], [PROJECT_ID], and [DATASET_ID] with your desired values.

Step 6: Verify that the user activity logging is enabled for BigQuery using the below command:

```
gcloud logging sinks describe [SINK_NAME]
```

This will display the details of the logging sink that you just created.

By following these steps, you can remediate the misconfiguration "GCP BigQuery Should Have User Activity Logging Enabled" for GCP using GCP CLI.
</Accordion>

<Accordion title='Using Python'>
To remediate the misconfiguration "GCP BigQuery Should Have User Activity Logging Enabled", you can follow these steps:

1. Open the GCP Console and navigate to the BigQuery service.
2. Click on the "Logs" tab in the left-hand menu.
3. Click on the "Audit Logs" tab.
4. Click on the "Create Sink" button.
5. Select the "BigQuery" destination.
6. Choose the project and dataset where you want to store the audit logs.
7. Click on the "Create Sink" button.
8. Open the Cloud Shell or terminal on your local machine and install the Google Cloud SDK.
9. Authenticate using your GCP account credentials by running the command `gcloud auth login`.
10. Set the project ID by running the command `gcloud config set project PROJECT_ID`.
11. Create a new Python file and import the necessary libraries:

```python
from google.cloud import logging_v2
from google.cloud.logging_v2 import enums
```

12. Initialize the Logging client:

```python
logging_client = logging_v2.LoggingServiceV2Client()
```

13. Define the BigQuery dataset ID and table ID where you want to store the audit logs:

```python
dataset_id = 'DATASET_ID'
table_id = 'TABLE_ID'
```

14. Define the Sink ID for the BigQuery sink:

```python
sink_name = f"projects/{project_id}/sinks/{sink_id}"
```

15. Create a new Sink object for the BigQuery sink:

```python
sink = logging_v2.LogSink(
    name=sink_name,
    destination=destination,
    filter_=filter_,
    output_version_format=enums.LogSink.VersionFormat.V2,
    bigquery_options=bigquery_options
)
```

16. Update the Sink configuration:

```python
update_mask = logging_v2.field_mask.FieldMask(
    paths=["destination", "filter", "bigquery_options"]
)

response = logging_client.update_sink(
    request={
        "sink_name": sink_name,
        "sink": sink,
        "update_mask": update_mask,
    }
)
```

17. Verify that the Sink configuration was updated successfully by checking the response:

```python
print(f"Sink updated: {response}")
```

18. Run the Python script to remediate the misconfiguration.

These steps will enable user activity logging for BigQuery on GCP and store the audit logs in a BigQuery dataset and table.
</Accordion>

</AccordionGroup>
</Tab>
</Tabs>
### Additional Reading:

- [https://cloud.google.com/bigquery/docs/reference/auditlogs](https://cloud.google.com/bigquery/docs/reference/auditlogs) 

