### Remediation

#### Using Console

To remediate the misconfiguration of GCP BigQuery not having audit logging enabled, you can follow the below steps in the GCP console:

1. Open the GCP Console and navigate to the BigQuery section.
2. Click on the name of the project that contains the BigQuery dataset that needs to be remediated.
3. On the left-hand side menu, select "IAM & Admin" and then select "Audit Logs".
4. In the Audit Logs page, click on the "ADD AUDIT LOGS" button.
5. In the "Add audit logs" page, select the "Data Access" option.
6. Under the "Service" section, select "BigQuery".
7. Under the "Logs" section, select the "Admin Read", "Data Read", and "Data Write" options.
8. Click on the "ADD" button to enable audit logging for BigQuery.
9. Wait for a few minutes for the changes to take effect.

Once the above steps are completed, audit logs will be enabled for the BigQuery dataset in the GCP project. You can now monitor and analyze the audit logs to identify any security issues or suspicious activity.

#### Using CLI

To remediate the misconfiguration of not having audit logging enabled in GCP BigQuery, you can follow these steps:

1. Open the Cloud Shell from the GCP console.

2. Run the following command to enable audit logging for BigQuery:

   ```
   gcloud logging sinks create [SINK-NAME] bigquery.googleapis.com/projects/[PROJECT-ID]/datasets/[DATASET-ID] --log-filter='protoPayload.serviceName="bigquery.googleapis.com" protoPayload.methodName="google.cloud.bigquery.v2.JobService.InsertJob" protoPayload.resourceName="projects/[PROJECT-ID]/jobs"'

   ```
   Note: Replace [SINK-NAME], [PROJECT-ID], and [DATASET-ID] with your own values.

3. After running the command, you will be prompted to authorize the Cloud Logging API to access your BigQuery dataset. Follow the prompts to complete the authorization.

4. Once the authorization is complete, the audit logs for BigQuery will be exported to your specified dataset.

5. You can verify that the audit logging is enabled by running the following command:

   ```
   gcloud logging sinks describe [SINK-NAME]
   ```
   This command will display the details of the sink you just created, including the filter and destination dataset.

6. Finally, you can test that the audit logging is working by performing an action in BigQuery, such as running a query, and then checking the logs in the destination dataset.

By following these steps, you can remediate the misconfiguration of not having audit logging enabled in GCP BigQuery.

#### Using Python

To remediate the misconfiguration of GCP BigQuery not having audit logging enabled using Python, you can follow these steps:

1. Import the necessary libraries:

```python
from google.cloud import bigquery_v2
from google.cloud.bigquery_v2.gapic.enums import (
    log_config_pb2,
    audit_log_config_pb2
)
```

2. Set up the BigQuery client:

```python
client = bigquery_v2.BigQueryLoggingServiceV2Client()
project_id = "YOUR_PROJECT_ID"
```

3. Check if audit logging is already enabled:

```python
parent = client.project_path(project_id)
response = client.get_bucket_iam_policy(request={"resource": parent})
```

4. If audit logging is not enabled, update the logging configuration:

```python
audit_config = audit_log_config_pb2.AuditLogConfig(
    log_types=[
        log_config_pb2.LogType.DATA_READ,
        log_config_pb2.LogType.DATA_WRITE,
        log_config_pb2.LogType.ADMIN_READ,
        log_config_pb2.LogType.ADMIN_WRITE,
        log_config_pb2.LogType.SYSTEM_EVENT,
    ]
)
update_mask = audit_log_config_pb2.AuditLogConfig(
    update_mask=audit_log_config_pb2.AuditLogConfig.LOG_TYPES_FIELD_NUMBER
)
client.update_bucket(request={"bucket": parent, "audit_config": audit_config, "update_mask": update_mask})
```

5. Verify that audit logging is now enabled:

```python
response = client.get_bucket_iam_policy(request={"resource": parent})
```

By following these steps, you can remediate the misconfiguration of GCP BigQuery not having audit logging enabled using Python.

