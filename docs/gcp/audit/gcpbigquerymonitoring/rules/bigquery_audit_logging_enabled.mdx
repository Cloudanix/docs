---
slug: bigquery_audit_logging_enabled
title: GCP BigQuery Should Have Audit Logging Enabled
sidebar_label: GCP BigQuery Should Have Audit Logging Enabled
---

### More Info:

Ensure that BigQuery Audit Logging is configured properly across all projects.

### Risk Level

Medium

### Address

Security

### Compliance Standards

HITRUST, SOC2, NISTCSF, PCIDSS


### Triage and Remediation
<Tabs>

<Tab title='Cause'>
### Check Cause
<AccordionGroup>
<Accordion title='Using Console' defaultOpen='true'>
1. Log in to the Google Cloud Platform Console: Open your web browser, navigate to the Google Cloud Platform Console (console.cloud.google.com), and sign in with your Google account credentials.

2. Navigate to BigQuery: From the left-hand side menu, select "BigQuery" under the "Big Data" section. This will take you to the BigQuery interface.

3. Check Dataset Audit Logs: Select a dataset from the resources panel on the left. In the details panel on the right, click on the "DATASET DETAILS" tab. Under this tab, look for the "Cloud Audit Logs" section. If audit logging is enabled, you should see entries for "Data Access" and "Admin Activity".

4. Check Project Audit Logs: Go back to the main GCP console, select "IAM & Admin" from the left-hand side menu, and then select "Audit logs". Here, you can check if audit logging is enabled at the project level. If it is, you should see entries for "Data Access" and "Admin Activity" for BigQuery.

Remember, these steps only allow you to check if audit logging is enabled. If you find that it's not, you'll need to follow additional steps to enable it.
</Accordion>

<Accordion title='Using CLI'>
1. First, you need to install and configure the Google Cloud SDK (gcloud CLI) on your local machine. You can download it from the official Google Cloud website and follow the instructions to install and authenticate it.

2. Once the Google Cloud SDK is installed and configured, open your terminal or command prompt and run the following command to list all the BigQuery datasets in your project:

   ```
   gcloud bigquery datasets list --project=[PROJECT_ID]
   ```

   Replace `[PROJECT_ID]` with your actual Google Cloud project ID.

3. For each dataset listed, you can check the audit logging configuration by running the following command:

   ```
   gcloud logging read "logName=projects/[PROJECT_ID]/logs/cloudaudit.googleapis.com%2Fdata_access AND resource.type=bigquery_resource AND protoPayload.serviceData.jobCompletedEvent.job.jobName.jobId=[JOB_ID]"
   ```

   Replace `[PROJECT_ID]` with your actual Google Cloud project ID and `[JOB_ID]` with the job ID of the BigQuery job you want to check.

4. If the command returns any logs, it means that audit logging is enabled for the specified BigQuery job. If it doesn't return any logs, it means that audit logging is not enabled.
</Accordion>

<Accordion title='Using Python'>
1. Install the necessary Python libraries: Google Cloud libraries are required to interact with GCP services. You can install it using pip:

```python
pip install google-cloud-logging
```

2. Import the necessary libraries and initialize the client:

```python
from google.cloud import logging
client = logging.Client()
```

3. Fetch the BigQuery datasets and check for audit logging:

```python
def check_bigquery_audit_logging():
    datasets = list(client.list_datasets())  # Make an API request.
    for dataset in datasets:
        dataset_id = dataset.dataset_id
        dataset_ref = client.dataset(dataset_id)
        dataset = client.get_dataset(dataset_ref)  # API request
        if 'cloudaudit.googleapis.com/activity' not in dataset.access_entries:
            print(f"Audit logging is not enabled for dataset: {dataset_id}")
```

4. Call the function to start the check:

```python
check_bigquery_audit_logging()
```

This script will print out the dataset IDs for which audit logging is not enabled. Please note that this script assumes that you have authenticated your GCP account on your local machine where this script is running. If not, you need to authenticate it using `gcloud auth application-default login` command.
</Accordion>

</AccordionGroup>
</Tab>

<Tab title='Remediation'>
### Remediation

<AccordionGroup>
<Accordion title='Using Console' defaultOpen='true'>
To remediate the misconfiguration "GCP BigQuery Should Have Audit Logging Enabled" for GCP using GCP console, you can follow the below steps:

1. Open the Google Cloud Console and select the project where BigQuery is enabled.

2. Go to the Navigation menu and select "BigQuery".

3. In the BigQuery console, click on the "More" button (three dots) on the left-hand side and select "View in APIs Explorer".

4. In the APIs Explorer, search for "tables.insert" in the search bar.

5. In the "tables.insert" API, scroll down to the "Request body" section and add the following JSON code:

```
{
  "tableReference": {
    "projectId": "project-id",
    "datasetId": "dataset-id",
    "tableId": "table-id"
  },
  "schema": {
    "fields": [
      {
        "name": "column1",
        "type": "STRING"
      },
      {
        "name": "column2",
        "type": "INTEGER"
      }
    ]
  },
  "labels": {
    "key": "value"
  },
  "timePartitioning": {
    "type": "DAY",
    "field": "timestamp"
  }
}
```

6. Click on the "Authorize and execute" button.

7. On the next screen, click on the "Execute" button.

8. Go back to the BigQuery console and click on the "More" button (three dots) on the left-hand side.

9. Select "Audit logs" and ensure that the logs are enabled.

By following these steps, you will remediate the misconfiguration "GCP BigQuery Should Have Audit Logging Enabled" for GCP using GCP console.

#
</Accordion>

<Accordion title='Using CLI'>
To remediate the misconfiguration of GCP BigQuery not having audit logging enabled, follow these steps using GCP CLI:

1. Open the Cloud Shell in your GCP console.

2. Run the following command to verify if audit logging is enabled for BigQuery:
```
gcloud logging logs list | grep bigquery
```
If you see any results, it means audit logging is already enabled. If not, proceed to the next step.

3. Run the following command to enable audit logging for BigQuery:
```
gcloud config set project <project-id>
gcloud services enable bigquery.googleapis.com
gcloud logging sinks create bigquery-audit \
  bigquery.googleapis.com/activity \
  --log-filter='protoPayload.serviceName="bigquery.googleapis.com"'
```
Note: Replace `<project-id>` with your GCP project ID.

4. Run the following command to verify if the sink was created successfully:
```
gcloud logging sinks list
```

5. Run the following command to grant the necessary permissions to the sink:
```
gcloud projects add-iam-policy-binding <project-id> \
  --member=serviceAccount:cloud-logs@system.gserviceaccount.com \
  --role=roles/bigquery.dataViewer
```

6. Run the following command to create a dataset in BigQuery to store the audit logs:
```
bq mk <dataset-name>
```
Note: Replace `<dataset-name>` with the desired name for your dataset.

7. Run the following command to create a table in the dataset to store the audit logs:
```
bq mk --table <dataset-name>.<table-name> \
  protoPayload \
  'timestamp:TIMESTAMP,protoPayload:STRING,severity:STRING,logName:STRING,resource:STRUCT<type:string,labels:map<string,string>>,operation:STRUCT<id:string,producer:string,first:BOOL,last:BOOL>,trace:STRING,spanId:STRING,receiveTimestamp:TIMESTAMP'
```
Note: Replace `<dataset-name>` and `<table-name>` with the desired names for your dataset and table.

8. Run the following command to create a sink to export the audit logs to the BigQuery table:
```
gcloud logging sinks create bigquery-audit \
  bigquery.googleapis.com/activity \
  --log-filter='protoPayload.serviceName="bigquery.googleapis.com"' \
  --destination=<project-id>:<dataset-name>.<table-name> \
  --include-children \
  --format='json'
```
Note: Replace `<project-id>`, `<dataset-name>` and `<table-name>` with the names you used in steps 6 and 7.

9. Run the following command to verify if the sink was created successfully:
```
gcloud logging sinks describe bigquery-audit
```

After following these steps, audit logging will be enabled for BigQuery in your GCP project, and the audit logs will be exported to the BigQuery table you created.
</Accordion>

<Accordion title='Using Python'>
To remediate the misconfiguration "GCP BigQuery should have audit logging enabled" for GCP using Python, follow the below steps:

1. Install the Google Cloud SDK and authenticate using the following command:
```
gcloud auth login
```

2. Install the necessary Python libraries:
```
pip install google-cloud-bigquery google-auth google-auth-oauthlib google-auth-httplib2
```

3. Create a Python script with the following code:
```python
from google.cloud import bigquery

client = bigquery.Client()

dataset_id = 'my_dataset'

dataset = client.get_dataset(dataset_id)

if not dataset.auditing_logs:
    dataset.auditing_logs = True
    dataset = client.update_dataset(dataset, ['auditing_logs'])

print('Audit logging enabled for dataset {}'.format(dataset_id))
```

4. Replace `my_dataset` with the ID of the dataset you want to enable audit logging for.

5. Run the script using the following command:
```
python script.py
```

This will enable audit logging for the specified dataset in GCP BigQuery.
</Accordion>

</AccordionGroup>
</Tab>
</Tabs>
### Additional Reading:

- [https://cloud.google.com/bigquery/docs/reference/auditlogs](https://cloud.google.com/bigquery/docs/reference/auditlogs) 

