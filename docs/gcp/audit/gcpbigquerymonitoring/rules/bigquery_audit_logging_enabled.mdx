---
slug: bigquery_audit_logging_enabled
title: GCP BigQuery Should Have Audit Logging Enabled
sidebar_label: GCP BigQuery Should Have Audit Logging Enabled
---

### More Info:

Ensure that BigQuery Audit Logging is configured properly across all projects.

### Risk Level

Medium

### Address

Security

### Compliance Standards

HITRUST, SOC2, NISTCSF, PCIDSS



<Tabs><Tab title='Cause'>
### Check Cause

#### Using Console

1. Log in to the Google Cloud Platform Console: Open your web browser, visit the Google Cloud Platform Console, and sign in with your Google account credentials.

2. Navigate to BigQuery: From the left-hand side menu, click on "BigQuery" under the "Big Data" section. This will take you to the BigQuery interface.

3. Check Dataset Audit Logs: In the BigQuery interface, select a dataset from the resources panel on the left. Once the dataset is selected, click on the "SHARE DATASET" button at the top of the right panel. In the "Share Dataset" window that appears, click on the "AUDIT LOGS" tab.

4. Verify Audit Logging: In the "AUDIT LOGS" tab, check if the "Data Access" and "Admin Read" logs are enabled. If they are, then audit logging is enabled for the selected BigQuery dataset. If not, then audit logging is not enabled, indicating a misconfiguration.

#### Using CLI

1. First, you need to install and initialize the Google Cloud SDK (Software Development Kit) if you haven't done so already. You can download it from the official Google Cloud website and follow the instructions provided there.

2. Once the SDK is installed, open your terminal or command prompt and authenticate your Google Cloud account by running the following command:
   ```
   gcloud auth login
   ```
   Follow the prompts to log in with your Google Cloud account.

3. Now, you can list all the datasets in your BigQuery by running the following command:
   ```
   gcloud bigquery datasets list
   ```
   This command will return a list of all datasets in your current project.

4. To check if audit logging is enabled for each dataset, you need to get the IAM policy for each dataset and check if the "cloudaudit.googleapis.com/data_access" role is included. You can do this by running the following command for each dataset:
   ```
   gcloud bigquery datasets get-iam-policy [DATASET_ID]
   ```
   Replace [DATASET_ID] with the ID of the dataset you want to check. If audit logging is enabled, you should see "cloudaudit.googleapis.com/data_access" in the list of roles. If it's not there, then audit logging is not enabled for that dataset.

#### Using Python

1. Install the necessary Python libraries: Google Cloud libraries are required to interact with GCP services. You can install it using pip:

```python
pip install google-cloud-logging
```

2. Import the necessary libraries and initialize the client:

```python
from google.cloud import logging
client = logging.Client()
```

3. Fetch the BigQuery datasets and check for audit logging: You can fetch all the datasets in your project and check if they have audit logging enabled. Here is a sample script:

```python
from google.cloud import bigquery

# Initialize a BigQuery client.
bq_client = bigquery.Client()

# List all the datasets in your project.
datasets = list(bq_client.list_datasets())

# Loop through each dataset.
for dataset in datasets:
    dataset_id = dataset.dataset_id
    # Get the dataset's metadata and check if audit logging is enabled.
    dataset_ref = bq_client.dataset(dataset_id)
    dataset = bq_client.get_dataset(dataset_ref)
    if 'audit' not in dataset.labels:
        print(f"Audit logging is not enabled for dataset {dataset_id}")
```

4. Analyze the output: If the script prints out any dataset IDs, it means those datasets do not have audit logging enabled. If no output is produced, it means all datasets have audit logging enabled. 

Please note that this script only checks for the existence of a label named 'audit' as a simple way to demonstrate how you might check for audit logging. In a real-world scenario, you would need to check the actual logging configuration of each dataset.

</Tab>


<Tab title='Remediation'>
### Remediation

#### Using Console

To remediate the misconfiguration "GCP BigQuery Should Have Audit Logging Enabled" for GCP using GCP console, you can follow the below steps:

1. Open the Google Cloud Console and select the project where BigQuery is enabled.

2. Go to the Navigation menu and select "BigQuery".

3. In the BigQuery console, click on the "More" button (three dots) on the left-hand side and select "View in APIs Explorer".

4. In the APIs Explorer, search for "tables.insert" in the search bar.

5. In the "tables.insert" API, scroll down to the "Request body" section and add the following JSON code:

```
{
  "tableReference": {
    "projectId": "project-id",
    "datasetId": "dataset-id",
    "tableId": "table-id"
  },
  "schema": {
    "fields": [
      {
        "name": "column1",
        "type": "STRING"
      },
      {
        "name": "column2",
        "type": "INTEGER"
      }
    ]
  },
  "labels": {
    "key": "value"
  },
  "timePartitioning": {
    "type": "DAY",
    "field": "timestamp"
  }
}
```

6. Click on the "Authorize and execute" button.

7. On the next screen, click on the "Execute" button.

8. Go back to the BigQuery console and click on the "More" button (three dots) on the left-hand side.

9. Select "Audit logs" and ensure that the logs are enabled.

By following these steps, you will remediate the misconfiguration "GCP BigQuery Should Have Audit Logging Enabled" for GCP using GCP console.

#### Using CLI

To remediate the misconfiguration of GCP BigQuery not having audit logging enabled, follow these steps using GCP CLI:

1. Open the Cloud Shell in your GCP console.

2. Run the following command to verify if audit logging is enabled for BigQuery:
```
gcloud logging logs list | grep bigquery
```
If you see any results, it means audit logging is already enabled. If not, proceed to the next step.

3. Run the following command to enable audit logging for BigQuery:
```
gcloud config set project <project-id>
gcloud services enable bigquery.googleapis.com
gcloud logging sinks create bigquery-audit \
  bigquery.googleapis.com/activity \
  --log-filter='protoPayload.serviceName="bigquery.googleapis.com"'
```
Note: Replace `<project-id>` with your GCP project ID.

4. Run the following command to verify if the sink was created successfully:
```
gcloud logging sinks list
```

5. Run the following command to grant the necessary permissions to the sink:
```
gcloud projects add-iam-policy-binding <project-id> \
  --member=serviceAccount:cloud-logs@system.gserviceaccount.com \
  --role=roles/bigquery.dataViewer
```

6. Run the following command to create a dataset in BigQuery to store the audit logs:
```
bq mk <dataset-name>
```
Note: Replace `<dataset-name>` with the desired name for your dataset.

7. Run the following command to create a table in the dataset to store the audit logs:
```
bq mk --table <dataset-name>.<table-name> \
  protoPayload \
  'timestamp:TIMESTAMP,protoPayload:STRING,severity:STRING,logName:STRING,resource:STRUCT<type:string,labels:map<string,string>>,operation:STRUCT<id:string,producer:string,first:BOOL,last:BOOL>,trace:STRING,spanId:STRING,receiveTimestamp:TIMESTAMP'
```
Note: Replace `<dataset-name>` and `<table-name>` with the desired names for your dataset and table.

8. Run the following command to create a sink to export the audit logs to the BigQuery table:
```
gcloud logging sinks create bigquery-audit \
  bigquery.googleapis.com/activity \
  --log-filter='protoPayload.serviceName="bigquery.googleapis.com"' \
  --destination=<project-id>:<dataset-name>.<table-name> \
  --include-children \
  --format='json'
```
Note: Replace `<project-id>`, `<dataset-name>` and `<table-name>` with the names you used in steps 6 and 7.

9. Run the following command to verify if the sink was created successfully:
```
gcloud logging sinks describe bigquery-audit
```

After following these steps, audit logging will be enabled for BigQuery in your GCP project, and the audit logs will be exported to the BigQuery table you created.

#### Using Python

To remediate the misconfiguration "GCP BigQuery should have audit logging enabled" for GCP using Python, follow the below steps:

1. Install the Google Cloud SDK and authenticate using the following command:
```
gcloud auth login
```

2. Install the necessary Python libraries:
```
pip install google-cloud-bigquery google-auth google-auth-oauthlib google-auth-httplib2
```

3. Create a Python script with the following code:
```python
from google.cloud import bigquery

client = bigquery.Client()

dataset_id = 'my_dataset'

dataset = client.get_dataset(dataset_id)

if not dataset.auditing_logs:
    dataset.auditing_logs = True
    dataset = client.update_dataset(dataset, ['auditing_logs'])

print('Audit logging enabled for dataset {}'.format(dataset_id))
```

4. Replace `my_dataset` with the ID of the dataset you want to enable audit logging for.

5. Run the script using the following command:
```
python script.py
```

This will enable audit logging for the specified dataset in GCP BigQuery.

### Additional Reading:

- [https://cloud.google.com/bigquery/docs/reference/auditlogs](https://cloud.google.com/bigquery/docs/reference/auditlogs) 


</Tab>
</Tabs>